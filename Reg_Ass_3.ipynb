{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "0dabf078",
   "metadata": {},
   "source": [
    "# question 1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "a56d91cc",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Ridge Regression is a type of regression analysis used when the dataset suffers from multicollinearity, meaning some of the independent variables are highly correlated. It's a regularization technique that penalizes the size of coefficients to prevent overfitting and reduce the influence of highly correlated variables.\n",
    "\n",
    "# The main difference between Ridge Regression and ordinary least squares (OLS) regression lies in the way they handle the coefficients. In OLS, the coefficients are estimated by minimizing the sum of squared differences between the observed and predicted values. However, in Ridge Regression, an additional penalty term is added to the OLS objective function, which includes the squared magnitudes of the coefficients multiplied by a tuning parameter (usually denoted as lambda or alpha). This penalty shrinks the coefficients towards zero, reducing their variance and, thus, reducing the model's sensitivity to multicollinearity."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f8aefef9",
   "metadata": {},
   "source": [
    "# question 2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "3a16a6b2",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "# The assumptions of Ridge Regression are similar to those of ordinary least squares (OLS) regression, with some additional considerations due to the regularization:\n",
    "\n",
    "# Linearity: The relationship between the independent variables and the dependent variable should be linear. Ridge Regression assumes that this relationship holds, just like OLS regression.\n",
    "\n",
    "# Independence: The observations should be independent of each other. This means that the value of one observation should not be influenced by the value of another observation.\n",
    "\n",
    "# Homoscedasticity: The variance of the errors should be constant across all levels of the independent variables. Ridge Regression does not directly assume homoscedasticity, but it helps in reducing the variance of the parameter estimates, which can indirectly improve homoscedasticity."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "58211e70",
   "metadata": {},
   "source": [
    "# question 3\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "19419e14",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Selecting the value of the tuning parameter (often denoted as lambda or alpha) in Ridge Regression is crucial for obtaining the best balance between bias and variance in the model. Here are several common methods for selecting the value of lambda:\n",
    "\n",
    "# Cross-Validation: One of the most widely used methods is cross-validation, particularly k-fold cross-validation. In this approach, the dataset is divided into k subsets, and the model is trained on k-1 subsets while validating on the remaining subset. This process is repeated k times, each time with a different validation subset. The average performance across all validation sets is used to select the best lambda value.\n",
    "\n",
    "# Grid Search: Grid search involves trying out a predefined set of lambda values and selecting the one that yields the best performance according to a chosen metric, such as mean squared error (MSE) or cross-validated MSE. This method involves testing lambda values across a grid of possible values and selecting the one with the lowest error.\n",
    "\n",
    "# Randomized Search: Similar to grid search but instead of exhaustively searching over a predefined grid of lambda values, randomized search samples lambda values randomly from a distribution. This approach can be more efficient than grid search, especially when the search space"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a6da7c3f",
   "metadata": {},
   "source": [
    "# question 4"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "c31b1f90",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "# Yes, Ridge Regression can be used for feature selection, although it does not inherently perform variable selection like some other methods such as Lasso Regression. However, Ridge Regression can indirectly aid in feature selection by shrinking the coefficients of less important features towards zero."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9f895e62",
   "metadata": {},
   "source": [
    "# question 5"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "3ef12e70",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Ridge Regression is particularly well-suited for dealing with multicollinearity, a situation where independent variables in a regression model are highly correlated with each other. In the presence of multicollinearity, ordinary least squares (OLS) regression estimates can become unstable or have inflated variances, leading to unreliable coefficient estimates. However, Ridge Regression effectively mitigates these issues by introducing a regularization term that penalizes the size of the coefficients."
   ]
  },
  {
   "cell_type": "raw",
   "id": "e4510fa0",
   "metadata": {},
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d34a2754",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
